#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Financial Data Processor with RAG capabilities.
- Financial data collection and preprocessing
- Vector database management using FAISS
- News and alert context generation
- RAG-based context retrieval for AI agents
- docs/data_processing.ipynb Í∏∞Î∞ò ÌÜµÌï©
"""

import logging
import json
import os
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
import numpy as np

# Disable all progress bars in sentence-transformers
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'

try:
    import faiss
except ImportError:
    faiss = None

try:
    from sentence_transformers import SentenceTransformer
    import warnings
    warnings.filterwarnings('ignore', category=FutureWarning)
except ImportError:
    SentenceTransformer = None

from config import config
import pandas as pd

# Configure logging to suppress progress bars
import transformers
transformers.logging.set_verbosity_error()

logger = logging.getLogger(__name__)

class FinancialDataProcessor:
    """Handles financial data processing and RAG context generation."""
    
    def __init__(self):
        # REQUIRED: Initialize embedding model
        self.embedding_model = self._load_embedding_model()
        
        # REQUIRED: Data caches - Initialize BEFORE vector_db
        self.news_cache: Dict[str, Any] = {}
        self.alerts_cache: Dict[str, Any] = {}
        self.market_data_cache: Dict[str, Any] = {}
        self.stock_evaluation_cache: Dict[str, Any] = {}
        
        # REQUIRED: Cache expiry times
        self.cache_expiry_hours = 1
        
        # REQUIRED: Initialize vector database
        self.vector_db = None
        self.document_store: List[Dict[str, Any]] = []
        self._initialize_vector_db()
        
        # Initialize stock search engine
        self.stock_search_engine = None
        self._initialize_stock_search()
        
        logger.info("FinancialDataProcessor initialized successfully")
    
    def _load_embedding_model(self) -> Optional[SentenceTransformer]:
        """
        Load sentence transformer model for embeddings.
        
        Returns:
            SentenceTransformer model or None if not available
        """
        if not SentenceTransformer:
            logger.warning("sentence-transformers not available, using mock embeddings")
            return None
        
        try:
            # REQUIRED: Use Korean-optimized model
            # Disable progress bars to avoid console spam
            import os
            os.environ['SENTENCE_TRANSFORMERS_HOME'] = os.path.join(os.path.dirname(__file__), '..', 'models')
            
            # Disable tqdm globally
            try:
                from tqdm import tqdm
                tqdm.pandas = lambda: None
                from functools import partialmethod
                tqdm.__init__ = partialmethod(tqdm.__init__, disable=True)
            except:
                pass
            
            model = SentenceTransformer('jhgan/ko-sbert-multitask', device='cpu')
            # Disable progress bars
            model.max_seq_length = 512
            model.eval()  # Set to evaluation mode
            
            logger.info("Financial embedding model loaded successfully")
            return model
        except Exception as e:
            logger.error(f"Failed to load embedding model: {e}")
            return None
    
    def _initialize_vector_db(self) -> None:
        """Initialize FAISS vector database."""
        try:
            if not faiss:
                logger.warning("FAISS not available, using simple search")
                return
            
            # REQUIRED: Initialize with 768-dimensional embeddings
            dimension = 768
            self.vector_db = faiss.IndexFlatL2(dimension)
            
            # REQUIRED: Load existing data if available
            self._load_existing_data()
            
            logger.info("Vector database initialized successfully")
            
        except Exception as e:
            logger.error(f"Vector database initialization failed: {e}")
            self.vector_db = None
    
    def _initialize_stock_search(self) -> None:
        """Initialize stock search engine with notebook-based TF-IDF."""
        try:
            from stock_search_engine import StockSearchEngine
            self.stock_search_engine = StockSearchEngine()
            # Load search index
            self.stock_search_engine.load_search_index()
            logger.info("Stock search engine initialized successfully")
        except Exception as e:
            logger.error(f"Stock search engine initialization failed: {e}")
            self.stock_search_engine = None
    
    def _load_existing_data(self) -> None:
        """Load existing financial data into vector database."""
        try:
            # SupabaseÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Î°úÎìú ÏãúÎèÑ
            from db_client import get_supabase_client
            supabase = get_supabase_client()
            
            if supabase:
                logger.info("Loading data from Supabase")
                self._load_supabase_data(supabase)
                return
            
            # SQLite Ìè¥Î∞±
            import sqlite3
            from pathlib import Path
            
            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í≤ΩÎ°ú ÏÑ§Ï†ï
            db_path = Path("investment_data.db")
            if not db_path.exists():
                db_path = Path("src") / "investment_data.db"
                if not db_path.exists():
                    logger.warning("Investment database not found. Loading mock data.")
                    self._load_mock_data()
                    return
            
            conn = sqlite3.connect(str(db_path))
            
            # ÏµúÏã† Îâ¥Ïä§ Î°úÎìú
            news_query = '''
                SELECT title, description as content, pub_date as timestamp,
                       sentiment_score, keywords
                FROM news_articles
                ORDER BY pub_date DESC
                LIMIT 100
            '''
            
            news_df = pd.read_sql_query(news_query, conn)
            
            for _, row in news_df.iterrows():
                try:
                    # ÌÇ§ÏõåÎìú ÌååÏã±
                    keywords = json.loads(row['keywords']) if row['keywords'] else []
                    
                    doc = {
                        "title": row['title'],
                        "content": row['content'] or '',
                        "category": "news",
                        "timestamp": row['timestamp'],
                        "sentiment_score": float(row['sentiment_score']) if row['sentiment_score'] else 0.5,
                        "relevance_keywords": keywords
                    }
                    self.add_document(doc)
                except Exception as e:
                    logger.error(f"Error adding news document: {e}")
                    continue
            
            # ÏúÑÌóò ÏïåÎ¶º ÏÉùÏÑ± (Ï£ºÍ∞Ä Í∏âÎ≥ÄÎèô Í∞êÏßÄ)
            alerts_query = '''
                WITH price_changes AS (
                    SELECT 
                        ticker,
                        date,
                        close,
                        LAG(close, 1) OVER (PARTITION BY ticker ORDER BY date) as prev_close,
                        (close - LAG(close, 1) OVER (PARTITION BY ticker ORDER BY date)) / 
                        LAG(close, 1) OVER (PARTITION BY ticker ORDER BY date) * 100 as change_pct
                    FROM korean_stock_prices
                    WHERE date >= date('now', '-7 days')
                )
                SELECT 
                    ticker,
                    MAX(ABS(change_pct)) as max_change,
                    AVG(ABS(change_pct)) as avg_volatility
                FROM price_changes
                WHERE change_pct IS NOT NULL
                GROUP BY ticker
                HAVING MAX(ABS(change_pct)) > 5
            '''
            
            try:
                alerts_df = pd.read_sql_query(alerts_query, conn)
                
                for _, row in alerts_df.iterrows():
                    severity = "high" if row['max_change'] > 10 else "medium"
                    
                    alert_doc = {
                        "title": f"{row['ticker']} Ï£ºÍ∞Ä Í∏âÎ≥ÄÎèô ÏïåÎ¶º",
                        "content": f"{row['ticker']} Ï¢ÖÎ™©Ïù¥ ÏµúÍ∑º {row['max_change']:.1f}% Î≥ÄÎèôÌñàÏäµÎãàÎã§. ÌèâÍ∑† Î≥ÄÎèôÏÑ±: {row['avg_volatility']:.1f}%",
                        "category": "alert",
                        "severity": severity,
                        "timestamp": datetime.now().isoformat(),
                        "relevance_keywords": [row['ticker'], "Î≥ÄÎèôÏÑ±", "Ï£ºÏùò"]
                    }
                    self.add_document(alert_doc)
                    
            except Exception as e:
                logger.error(f"Error generating alerts: {e}")
            
            conn.close()
            
            logger.info(f"Loaded {len(self.document_store)} documents from database")
            
        except Exception as e:
            logger.error(f"Failed to load existing data: {e}")
            self._load_mock_data()
    
    def _load_supabase_data(self, supabase) -> None:
        """Load financial data from Supabase."""
        try:
            # ÏµúÏã† ÌïúÍµ≠ Ï£ºÏãù Í∞ÄÍ≤© Îç∞Ïù¥ÌÑ∞ Î°úÎìú
            today = datetime.now()
            week_ago = today - timedelta(days=7)
            
            # ÌïúÍµ≠ Ï£ºÏãù Í∞ÄÍ≤© Îç∞Ïù¥ÌÑ∞
            try:
                kor_prices = supabase.table("kor_stock_prices").select("*").gte(
                    "date", week_ago.strftime("%Y-%m-%d")
                ).execute()
                
                if kor_prices.data:
                    logger.info(f"Loaded {len(kor_prices.data)} Korean stock price records")
                    # Í∞ÄÍ≤© Îç∞Ïù¥ÌÑ∞Î•º Î¨∏ÏÑúÎ°ú Î≥ÄÌôò
                    for price in kor_prices.data[:50]:  # ÏµúÍ∑º 50Í∞úÎßå
                        doc = {
                            "title": f"{price.get('name', price['ticker'])} Ï£ºÍ∞Ä Ï†ïÎ≥¥",
                            "content": f"{price['ticker']} Ï¢ÖÍ∞Ä: {price['close']:,.0f}Ïõê, Í±∞ÎûòÎüâ: {price['volume']:,}",
                            "category": "stock_price",
                            "timestamp": price['date'],
                            "ticker": price['ticker'],
                            "close": price['close'],
                            "volume": price['volume'],
                            "relevance_keywords": [price['ticker'], price.get('name', ''), "Ï£ºÍ∞Ä"]
                        }
                        self.add_document(doc)
                        # Ï∫êÏãúÏóêÎèÑ Ï†ÄÏû•
                        self.market_data_cache[price['ticker']] = price
            except Exception as e:
                logger.error(f"Error loading Korean stock prices: {e}")
            
            # ÌïúÍµ≠ Ï£ºÏãù Î∞∏Î•òÏóêÏù¥ÏÖò ÏßÄÌëú
            try:
                kor_valuations = supabase.table("kor_valuation_metrics").select("*").gte(
                    "date", week_ago.strftime("%Y-%m-%d")
                ).execute()
                
                if kor_valuations.data:
                    logger.info(f"Loaded {len(kor_valuations.data)} valuation records")
                    # Ìã∞Ïª§Î≥ÑÎ°ú ÏµúÏã† ÏßÄÌëúÎßå Ï†ÄÏû•
                    ticker_valuations = {}
                    for val in kor_valuations.data:
                        ticker = val['ticker']
                        if ticker not in ticker_valuations:
                            ticker_valuations[ticker] = {}
                        ticker_valuations[ticker][val['metric']] = val['value']
                    
                    # Î∞∏Î•òÏóêÏù¥ÏÖò Ï∫êÏãú ÏóÖÎç∞Ïù¥Ìä∏
                    for ticker, metrics in ticker_valuations.items():
                        if ticker in self.market_data_cache:
                            self.market_data_cache[ticker]['valuations'] = metrics
            except Exception as e:
                logger.error(f"Error loading valuations: {e}")
            
            # ÎØ∏Íµ≠ Ï£ºÏãù Í∞ÄÍ≤© Îç∞Ïù¥ÌÑ∞
            try:
                us_prices = supabase.table("us_stock_prices").select("*").gte(
                    "date", week_ago.strftime("%Y-%m-%d")
                ).execute()
                
                if us_prices.data:
                    logger.info(f"Loaded {len(us_prices.data)} US stock price records")
                    for price in us_prices.data[:20]:  # ÏµúÍ∑º 20Í∞úÎßå
                        doc = {
                            "title": f"{price['ticker']} Stock Price",
                            "content": f"{price['ticker']} Close: ${price['close']:.2f}, Volume: {price['volume']:,}",
                            "category": "stock_price",
                            "timestamp": price['date'],
                            "ticker": price['ticker'],
                            "close": price['close'],
                            "volume": price['volume'],
                            "relevance_keywords": [price['ticker'], "US", "stock"]
                        }
                        self.add_document(doc)
            except Exception as e:
                logger.error(f"Error loading US stock prices: {e}")
            
            # ÌïúÍµ≠ Ï£ºÏãù ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞
            try:
                kor_evaluations = supabase.table("kor_stock_evaluations").select("*").execute()
                
                if kor_evaluations.data:
                    logger.info(f"Loaded {len(kor_evaluations.data)} Korean stock evaluation records")
                    for eval_data in kor_evaluations.data:
                        # ÌèâÍ∞Ä Ï∫êÏãúÏóê Ï†ÄÏû•
                        self.stock_evaluation_cache[eval_data['ticker']] = {
                            'name': eval_data['name'],
                            'score': eval_data['score'],
                            'evaluation': eval_data['evaluation'],
                            'per': eval_data['per'],
                            'pbr': eval_data['pbr'],
                            'reasons': eval_data['reasons']
                        }
                        
                        # ÎÜíÏùÄ ÌèâÍ∞Ä Ï†êÏàòÎ•º Î∞õÏùÄ Ï¢ÖÎ™©ÏùÄ Î¨∏ÏÑúÎ°úÎèÑ Ï∂îÍ∞Ä
                        if eval_data['score'] >= 70:
                            doc = {
                                "title": f"{eval_data['name']} Ìà¨Ïûê Ï∂îÏ≤ú",
                                "content": f"{eval_data['name']}({eval_data['ticker']}) - ÌèâÍ∞ÄÏ†êÏàò: {eval_data['score']}Ï†ê, {eval_data['evaluation']}. {eval_data['reasons']}",
                                "category": "stock_recommendation",
                                "timestamp": eval_data.get('created_at', datetime.now().isoformat()),
                                "ticker": eval_data['ticker'],
                                "score": eval_data['score'],
                                "relevance_keywords": [eval_data['ticker'], eval_data['name'], "Ï∂îÏ≤ú", eval_data['evaluation']]
                            }
                            self.add_document(doc)
            except Exception as e:
                logger.error(f"Error loading Korean stock evaluations: {e}")
            
            # Îâ¥Ïä§ Îç∞Ïù¥ÌÑ∞ (ÏÉàÎ°úÏö¥ ÌÖåÏù¥Î∏îÏù¥ ÏûàÎã§Î©¥)
            try:
                news = supabase.table("news").select("*").gte(
                    "created_at", week_ago.isoformat()
                ).limit(50).execute()
                
                if news.data:
                    logger.info(f"Loaded {len(news.data)} news articles")
                    for article in news.data:
                        doc = {
                            "title": article.get('title', ''),
                            "content": article.get('content', ''),
                            "category": "news",
                            "timestamp": article.get('created_at', ''),
                            "sentiment_score": article.get('sentiment_score', 0.5),
                            "relevance_keywords": article.get('keywords', [])
                        }
                        self.add_document(doc)
            except Exception as e:
                logger.info(f"News table not found or error: {e}")
            
            logger.info(f"Total documents loaded from Supabase: {len(self.document_store)}")
            
        except Exception as e:
            logger.error(f"Failed to load Supabase data: {e}")
            self._load_mock_data()
    
    def _load_mock_data(self) -> None:
        """Load mock data as fallback"""
        mock_news = [
            {
                "title": "ÌïúÍµ≠ Ï¶ùÏãú ÏÉÅÏäπÏÑ∏ ÏßÄÏÜç",
                "content": "ÏΩîÏä§ÌîºÍ∞Ä 3Ïùº Ïó∞ÏÜç ÏÉÅÏäπÌïòÎ©∞ 2400ÎåÄÎ•º ÌöåÎ≥µÌñàÏäµÎãàÎã§. Í∏∞Ïà†Ï£º Ï§ëÏã¨Ïùò Îß§ÏàòÏÑ∏Í∞Ä Ïù¥Ïñ¥ÏßÄÍ≥† ÏûàÏäµÎãàÎã§.",
                "category": "news",
                "timestamp": datetime.now().isoformat(),
                "sentiment_score": 0.8,
                "relevance_keywords": ["ÏΩîÏä§Ìîº", "ÏÉÅÏäπ", "Í∏∞Ïà†Ï£º"]
            },
            {
                "title": "ÎØ∏Íµ≠ Ïó∞Ï§Ä Í∏àÎ¶¨ Ïù∏Ìïò ÏãúÏÇ¨",
                "content": "ÎØ∏Íµ≠ Ïó∞Î∞©Ï§ÄÎπÑÏ†úÎèÑÍ∞Ä Îã§Ïùå ÌöåÏùòÏóêÏÑú Í∏àÎ¶¨ Ïù∏ÌïòÎ•º Í≤ÄÌÜ†Ìï† Í≤ÉÏù¥ÎùºÍ≥† Î∞úÌëúÌñàÏäµÎãàÎã§.",
                "category": "news",
                "timestamp": datetime.now().isoformat(),
                "sentiment_score": 0.7,
                "relevance_keywords": ["Ïó∞Ï§Ä", "Í∏àÎ¶¨", "Ïù∏Ìïò"]
            }
        ]
        
        for doc in mock_news:
            self.add_document(doc)
        
        logger.info("Loaded mock data as fallback")
    
    def add_document(self, document: Dict[str, Any]) -> bool:
        """
        Add document to vector database.
        
        Args:
            document: Document with title, content, category, etc.
            
        Returns:
            True if added successfully
        """
        try:
            if not self.embedding_model:
                # REQUIRED: Store without embeddings if model unavailable
                self.document_store.append(document)
                return True
            
            # REQUIRED: Generate embedding
            text = f"{document.get('title', '')} {document.get('content', '')}"
            # Disable progress bar for encoding
            embedding = self.embedding_model.encode(text, show_progress_bar=False, batch_size=1)
            
            # REQUIRED: Add to vector database
            if self.vector_db is not None:
                self.vector_db.add(embedding.reshape(1, -1))
            
            # REQUIRED: Store document with metadata
            document_with_id = {
                **document,
                "doc_id": len(self.document_store),
                "embedding": embedding.tolist() if hasattr(embedding, 'tolist') else None
            }
            
            self.document_store.append(document_with_id)
            
            logger.debug(f"Added document: {document.get('title', 'Untitled')}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to add document: {e}")
            return False
    
    def search_relevant_documents(
        self, 
        query: str, 
        top_k: int = 5, 
        category_filter: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Search for relevant documents using vector similarity.
        
        Args:
            query: Search query
            top_k: Number of top results to return
            category_filter: Filter by document category
            
        Returns:
            List of relevant documents
        """
        try:
            if not self.document_store:
                return []
            
            # REQUIRED: Vector search if available
            if self.embedding_model and self.vector_db is not None:
                return self._vector_search(query, top_k, category_filter)
            else:
                return self._keyword_search(query, top_k, category_filter)
                
        except Exception as e:
            logger.error(f"Document search failed: {e}")
            return []
    
    def _vector_search(
        self, 
        query: str, 
        top_k: int, 
        category_filter: Optional[str]
    ) -> List[Dict[str, Any]]:
        """Perform vector similarity search."""
        try:
            # REQUIRED: Generate query embedding
            query_embedding = self.embedding_model.encode(query, show_progress_bar=False, batch_size=1)
            
            # REQUIRED: Search in vector database
            distances, indices = self.vector_db.search(query_embedding.reshape(1, -1), top_k * 2)
            
            # REQUIRED: Get documents and apply filters
            results = []
            for i, doc_idx in enumerate(indices[0]):
                if doc_idx < len(self.document_store):
                    doc = self.document_store[doc_idx]
                    
                    # REQUIRED: Apply category filter
                    if category_filter and doc.get("category") != category_filter:
                        continue
                    
                    # REQUIRED: Add similarity score
                    doc_with_score = {
                        **doc,
                        "similarity_score": float(1.0 / (1.0 + distances[0][i]))
                    }
                    
                    results.append(doc_with_score)
                    
                    if len(results) >= top_k:
                        break
            
            return results
            
        except Exception as e:
            logger.error(f"Vector search failed: {e}")
            return []
    
    def _keyword_search(
        self, 
        query: str, 
        top_k: int, 
        category_filter: Optional[str]
    ) -> List[Dict[str, Any]]:
        """Perform simple keyword search as fallback."""
        try:
            query_terms = query.lower().split()
            scored_docs = []
            
            for doc in self.document_store:
                # REQUIRED: Apply category filter
                if category_filter and doc.get("category") != category_filter:
                    continue
                
                # REQUIRED: Calculate keyword score
                text = f"{doc.get('title', '')} {doc.get('content', '')}".lower()
                score = sum(1 for term in query_terms if term in text)
                
                if score > 0:
                    scored_docs.append({
                        **doc,
                        "keyword_score": score
                    })
            
            # REQUIRED: Sort by score and return top_k
            scored_docs.sort(key=lambda x: x["keyword_score"], reverse=True)
            return scored_docs[:top_k]
            
        except Exception as e:
            logger.error(f"Keyword search failed: {e}")
            return []
    
    def get_news_context(self, user_query: str) -> str:
        """
        Get news context relevant to user query.
        
        Args:
            user_query: User's query for context relevance
            
        Returns:
            Formatted news context string
        """
        try:
            # REQUIRED: Check cache first
            cache_key = f"news_{hash(user_query)}"
            cached_result = self._get_cached_result(cache_key, "news")
            if cached_result:
                return cached_result
            
            # REQUIRED: Search for relevant news
            news_docs = self.search_relevant_documents(
                user_query, 
                top_k=5, 
                category_filter="news"
            )
            
            if not news_docs:
                context = "[ÏµúÏã† Îâ¥Ïä§]\nÍ¥ÄÎ†® Îâ¥Ïä§Í∞Ä ÏóÜÏäµÎãàÎã§."
            else:
                news_items = []
                sentiment_scores = []
                
                for doc in news_docs:
                    title = doc.get("title", "Ï†úÎ™© ÏóÜÏùå")
                    content = doc.get("content", "")[:100] + "..."
                    sentiment_score = doc.get("sentiment_score", 0.5)
                    
                    # Í∞êÏ†ï Ï†êÏàòÏóê Îî∞Î•∏ Î†àÏù¥Î∏î
                    if sentiment_score > 0.7:
                        sentiment_label = "Í∏çÏ†ï"
                    elif sentiment_score < 0.3:
                        sentiment_label = "Î∂ÄÏ†ï"
                    else:
                        sentiment_label = "Ï§ëÎ¶Ω"
                    
                    news_items.append(f"- [{sentiment_label}] {title}: {content}")
                    sentiment_scores.append(sentiment_score)
                
                # Ï†ÑÏ≤¥ Í∞êÏ†ï Î∂ÑÏÑù ÏöîÏïΩ
                avg_sentiment = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0.5
                positive_count = sum(1 for s in sentiment_scores if s > 0.7)
                negative_count = sum(1 for s in sentiment_scores if s < 0.3)
                
                context = "[ÏµúÏã† Îâ¥Ïä§]\n" + "\n".join(news_items)
                context += f"\n\n[Í∞êÏ†ï Î∂ÑÏÑù ÏöîÏïΩ] Í∏çÏ†ï: {positive_count}Í∞ú, Î∂ÄÏ†ï: {negative_count}Í∞ú, ÌèâÍ∑† Ï†êÏàò: {avg_sentiment:.2f}"
            
            # REQUIRED: Cache result
            self._cache_result(cache_key, "news", context)
            
            return context
            
        except Exception as e:
            logger.error(f"Failed to get news context: {e}")
            return "[ÏµúÏã† Îâ¥Ïä§]\nÎâ¥Ïä§ Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§."
    
    def get_risk_alerts_context(self, user_query: str) -> str:
        """
        Get risk alerts context relevant to user query.
        
        Args:
            user_query: User's query for context relevance
            
        Returns:
            Formatted alerts context string
        """
        try:
            # REQUIRED: Check cache first
            cache_key = f"alerts_{hash(user_query)}"
            cached_result = self._get_cached_result(cache_key, "alerts")
            if cached_result:
                return cached_result
            
            # REQUIRED: Search for relevant alerts
            alert_docs = self.search_relevant_documents(
                user_query,
                top_k=5,
                category_filter="alert"
            )
            
            if not alert_docs:
                context = "[ÏúÑÌóò ÏïåÎ¶º]\nÌòÑÏû¨ ÌäπÎ≥ÑÌïú ÏúÑÌóò ÏöîÏÜåÍ∞Ä ÏóÜÏäµÎãàÎã§."
            else:
                alert_items = []
                for doc in alert_docs:
                    title = doc.get("title", "ÏïåÎ¶º ÏóÜÏùå")
                    content = doc.get("content", "")[:100] + "..."
                    severity = doc.get("severity", "medium")
                    severity_icon = "üî¥" if severity == "high" else "üü°" if severity == "medium" else "üü¢"
                    alert_items.append(f"{severity_icon} {title}: {content}")
                
                context = "[ÏúÑÌóò ÏïåÎ¶º]\n" + "\n".join(alert_items)
            
            # REQUIRED: Cache result  
            self._cache_result(cache_key, "alerts", context)
            
            return context
            
        except Exception as e:
            logger.error(f"Failed to get alerts context: {e}")
            return "[ÏúÑÌóò ÏïåÎ¶º]\nÏïåÎ¶º Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§."
    
    def get_stock_evaluation_context(self, user_query: str) -> str:
        """
        Get stock evaluation context using notebook-based search engine.
        
        Args:
            user_query: User's query for stock recommendations
            
        Returns:
            Formatted stock evaluation context string
        """
        try:
            if not self.stock_search_engine:
                return "[Ï£ºÏãù ÌèâÍ∞Ä]\nÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞Î•º ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§."
            
            # Check cache first
            cache_key = f"stock_eval_{hash(user_query)}"
            cached_result = self._get_cached_result(cache_key, "stock_evaluation")
            if cached_result:
                return cached_result
            
            # Search stocks using TF-IDF search engine
            search_results = self.stock_search_engine.search_stocks(user_query, n_results=5)
            
            if not search_results:
                context = "[Ï£ºÏãù Ï∂îÏ≤ú Í≤∞Í≥º]\nÍ¥ÄÎ†® Ï¢ÖÎ™©ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
            else:
                context_items = []
                context_items.append("[Ï£ºÏãù Ï∂îÏ≤ú Í≤∞Í≥º]")
                
                for i, result in enumerate(search_results, 1):
                    stock_info = []
                    stock_info.append(f"\n{i}. {result.Ï¢ÖÎ™©Î™Ö} ({result.Ï¢ÖÎ™©ÏΩîÎìú})")
                    stock_info.append(f"   - ÌòÑÏû¨Í∞Ä: {result.ÌòÑÏû¨Í∞Ä:,.0f}Ïõê")
                    stock_info.append(f"   - ÏãúÍ∞ÄÏ¥ùÏï°: {result.ÏãúÍ∞ÄÏ¥ùÏï°/100000000:,.0f}ÏñµÏõê")
                    stock_info.append(f"   - ÌèâÍ∞ÄÏ†êÏàò: {result.ÌèâÍ∞ÄÏ†êÏàò}Ï†ê")
                    stock_info.append(f"   - Ï¢ÖÌï©ÌèâÍ∞Ä: {result.Ï¢ÖÌï©ÌèâÍ∞Ä}")
                    
                    # Ïû¨Î¨¥ ÏßÄÌëú Ï∂îÍ∞Ä
                    if result.PER is not None:
                        stock_info.append(f"   - PER: {result.PER:.1f}, PBR: {result.PBR:.1f}")
                    if result.Îß§Ï∂úÏÑ±Ïû•Î•† is not None:
                        stock_info.append(f"   - Îß§Ï∂úÏÑ±Ïû•Î•†: {result.Îß§Ï∂úÏÑ±Ïû•Î•†:.1f}%")
                    if result.ÏàúÏù¥ÏùµÎ•† is not None:
                        stock_info.append(f"   - ÏàúÏù¥ÏùµÎ•†: {result.ÏàúÏù¥ÏùµÎ•†:.1f}%")
                    if result.Î∂ÄÏ±ÑÎπÑÏú® is not None:
                        stock_info.append(f"   - Î∂ÄÏ±ÑÎπÑÏú®: {result.Î∂ÄÏ±ÑÎπÑÏú®:.1f}%")
                    
                    # ÌèâÍ∞ÄÏù¥Ïú† Ï∂îÍ∞Ä (stock_evaluation_results.csvÏóêÏÑú)
                    if hasattr(result, 'ÌèâÍ∞ÄÏù¥Ïú†'):
                        stock_info.append(f"   - ÌèâÍ∞ÄÏù¥Ïú†: {result.ÌèâÍ∞ÄÏù¥Ïú†}")
                    else:
                        # SupabaseÏóêÏÑú ÌèâÍ∞ÄÏù¥Ïú† Í∞ÄÏ†∏Ïò§Í∏∞
                        if result.Ï¢ÖÎ™©ÏΩîÎìú in self.stock_evaluation_cache:
                            reasons = self.stock_evaluation_cache[result.Ï¢ÖÎ™©ÏΩîÎìú].get('reasons', '')
                            if reasons:
                                stock_info.append(f"   - ÌèâÍ∞ÄÏù¥Ïú†: {reasons}")
                    
                    stock_info.append(f"   - Í∏∞Ï§ÄÏùº: 2025ÎÖÑ 8Ïõî 1Ïùº")
                    
                    context_items.extend(stock_info)
                
                context = "\n".join(context_items)
            
            # Cache result
            self._cache_result(cache_key, "stock_evaluation", context)
            
            return context
            
        except Exception as e:
            logger.error(f"Failed to get stock evaluation context: {e}")
            return "[Ï£ºÏãù ÌèâÍ∞Ä]\nÌèâÍ∞Ä Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§."
    
    def get_top_stocks_context(self, criteria: str = "ÌèâÍ∞ÄÏ†êÏàò", n_results: int = 10) -> str:
        """
        Get top stocks based on evaluation criteria.
        
        Args:
            criteria: Evaluation criteria (ÌèâÍ∞ÄÏ†êÏàò, Îß§Ï∂úÏÑ±Ïû•Î•†, etc.)
            n_results: Number of top stocks to return
            
        Returns:
            Formatted top stocks context string
        """
        try:
            if not self.stock_search_engine:
                return "[ÏÉÅÏúÑ Ï¢ÖÎ™©]\nÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞Î•º ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§."
            
            # Get top stocks
            top_stocks = self.stock_search_engine.get_top_stocks(n_results, criteria)
            
            if not top_stocks:
                context = f"[{criteria} ÏÉÅÏúÑ Ï¢ÖÎ™©]\nÏÉÅÏúÑ Ï¢ÖÎ™©ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
            else:
                context_items = []
                context_items.append(f"[{criteria} ÏÉÅÏúÑ {n_results}Í∞ú Ï¢ÖÎ™©]")
                
                for i, stock in enumerate(top_stocks, 1):
                    context_items.append(f"{i}. {stock.Ï¢ÖÎ™©Î™Ö} ({stock.Ï¢ÖÎ™©ÏΩîÎìú}): {getattr(stock, criteria) if hasattr(stock, criteria) else stock.ÌèâÍ∞ÄÏ†êÏàò}")
                
                context = "\n".join(context_items)
            
            return context
            
        except Exception as e:
            logger.error(f"Failed to get top stocks context: {e}")
            return "[ÏÉÅÏúÑ Ï¢ÖÎ™©]\nÏÉÅÏúÑ Ï¢ÖÎ™© Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§."
    
    def get_market_data_context(self, user_query: str) -> str:
        """
        Get market data context relevant to user query including news sentiment.
        
        Args:
            user_query: User's query for context relevance
            
        Returns:
            Formatted market data context string with sentiment analysis
        """
        try:
            # REQUIRED: Check cache first
            cache_key = f"market_{hash(user_query)}"
            cached_result = self._get_cached_result(cache_key, "market")
            if cached_result:
                return cached_result
            
            # Îâ¥Ïä§ Í∞êÏ†ï Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ Í∞ÄÏ†∏Ïò§Í∏∞
            news_sentiment_context = ""
            try:
                # ÏµúÏã† Îâ¥Ïä§ Îç∞Ïù¥ÌÑ∞ Î°úÎìú
                import os
                from datetime import datetime
                import pandas as pd
                from pathlib import Path
                
                # Îç∞Ïù¥ÌÑ∞ ÎîîÎ†âÌÜ†Î¶¨ Í≤ΩÎ°ú
                data_dir = Path(__file__).parent.parent / "data" / "raw"
                today = datetime.now().strftime('%Y%m%d')
                news_file = data_dir / f"news_{today}.csv"
                
                # Ïò§Îäò ÌååÏùºÏù¥ ÏóÜÏúºÎ©¥ Í∞ÄÏû• ÏµúÍ∑º ÌååÏùº Ï∞æÍ∏∞
                if not news_file.exists():
                    news_files = list(data_dir.glob("news_*.csv"))
                    if news_files:
                        news_file = max(news_files, key=lambda x: x.stat().st_mtime)
                
                if news_file.exists():
                    # Îâ¥Ïä§ Îç∞Ïù¥ÌÑ∞ Î°úÎìú
                    news_df = pd.read_csv(news_file, encoding='utf-8')
                    
                    # Í∞êÏ†ï Î∂ÑÏÑù ÏàòÌñâ
                    import sys
                    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
                    from news_sentiment_analyzer import NewsSentimentAnalyzer
                    analyzer = NewsSentimentAnalyzer()
                    sentiment_result = analyzer.analyze_news_sentiment(news_df)
                    
                    # Í∞êÏ†ï Î∂ÑÏÑù Ïª®ÌÖçÏä§Ìä∏ ÏÉùÏÑ±
                    market_mood = sentiment_result['market_mood']
                    sentiment_dist = sentiment_result['sentiment_distribution']
                    
                    news_sentiment_context = f"""
[Îâ¥Ïä§ Í∏∞Î∞ò ÏãúÏû• Î∂ÑÏúÑÍ∏∞]
- Ï†ÑÏ≤¥ Í∞êÏ†ï Ï†êÏàò: {sentiment_result['overall_sentiment']:.2f} ({market_mood['mood']})
- Í∞êÏ†ï Î∂ÑÌè¨: Í∏çÏ†ï {sentiment_dist['positive']}Í±¥, Ï§ëÎ¶Ω {sentiment_dist['neutral']}Í±¥, Î∂ÄÏ†ï {sentiment_dist['negative']}Í±¥
- ÏãúÏû• ÏÑ§Î™Ö: {market_mood['description']}
- Ìà¨Ïûê Í∂åÍ≥†: {market_mood['recommendation']}
"""
                    
                    # Ï£ºÏöî ÌÇ§ÏõåÎìú Ï∂îÍ∞Ä
                    if sentiment_result['top_positive_keywords']:
                        pos_keywords = [f"{kw[0]}({kw[1]})" for kw in sentiment_result['top_positive_keywords'][:5]]
                        news_sentiment_context += f"- Í∏çÏ†ï ÌÇ§ÏõåÎìú: {', '.join(pos_keywords)}\n"
                    
                    if sentiment_result['top_negative_keywords']:
                        neg_keywords = [f"{kw[0]}({kw[1]})" for kw in sentiment_result['top_negative_keywords'][:5]]
                        news_sentiment_context += f"- Î∂ÄÏ†ï ÌÇ§ÏõåÎìú: {', '.join(neg_keywords)}\n"
                    
                    # Ï¢ÖÎ™©Î≥Ñ Í∞êÏ†ï Î∂ÑÏÑù
                    if sentiment_result['stock_sentiments']:
                        stock_sentiments = []
                        for stock, data in sentiment_result['stock_sentiments'].items():
                            if data['count'] > 0:
                                sentiment_icon = "üìà" if data['trend'] == 'positive' else "üìâ" if data['trend'] == 'negative' else "‚û°Ô∏è"
                                stock_sentiments.append(f"{stock}{sentiment_icon}({data['sentiment']:.2f})")
                        
                        if stock_sentiments:
                            news_sentiment_context += f"- Ï¢ÖÎ™©Î≥Ñ Í∞êÏ†ï: {', '.join(stock_sentiments[:5])}\n"
                            
            except Exception as e:
                logger.warning(f"Failed to get news sentiment: {e}")
                news_sentiment_context = ""
            
            # PLACEHOLDER: Mock market data
            # In real implementation, this would fetch from financial APIs
            market_data = {
                "KOSPI": {"value": 2400.50, "change": "+1.2%"},
                "KOSDAQ": {"value": 850.30, "change": "+0.8%"},
                "USD/KRW": {"value": 1345.20, "change": "+0.3%"},
                "VIX": {"value": 18.5, "change": "-2.1%"}
            }
            
            context_items = []
            for symbol, data in market_data.items():
                context_items.append(f"- {symbol}: {data['value']} ({data['change']})")
            
            context = "[ÏãúÏû• Îç∞Ïù¥ÌÑ∞]\n" + "\n".join(context_items)
            
            # Îâ¥Ïä§ Í∞êÏ†ï Î∂ÑÏÑù Ïª®ÌÖçÏä§Ìä∏ Ï∂îÍ∞Ä
            if news_sentiment_context:
                context += "\n\n" + news_sentiment_context
            
            # REQUIRED: Cache result
            self._cache_result(cache_key, "market", context)
            
            return context
            
        except Exception as e:
            logger.error(f"Failed to get market data context: {e}")
            return "[ÏãúÏû• Îç∞Ïù¥ÌÑ∞]\nÏãúÏû• Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§."
    
    def _get_cached_result(self, cache_key: str, cache_type: str) -> Optional[str]:
        """Get cached result if still valid."""
        try:
            cache_dict = getattr(self, f"{cache_type}_cache", {})
            
            if cache_key in cache_dict:
                cached_item = cache_dict[cache_key]
                cache_time = datetime.fromisoformat(cached_item["timestamp"])
                
                # REQUIRED: Check if cache is still valid
                if datetime.now() - cache_time < timedelta(hours=self.cache_expiry_hours):
                    return cached_item["data"]
                else:
                    # REQUIRED: Remove expired cache
                    del cache_dict[cache_key]
            
            return None
            
        except Exception as e:
            logger.error(f"Cache retrieval failed: {e}")
            return None
    
    def _cache_result(self, cache_key: str, cache_type: str, data: str) -> None:
        """Cache result with timestamp."""
        try:
            cache_dict = getattr(self, f"{cache_type}_cache", {})
            
            cache_dict[cache_key] = {
                "data": data,
                "timestamp": datetime.now().isoformat()
            }
            
            # REQUIRED: Limit cache size
            if len(cache_dict) > 100:
                # Remove oldest entries
                sorted_items = sorted(
                    cache_dict.items(),
                    key=lambda x: x[1]["timestamp"]
                )
                
                for key, _ in sorted_items[:50]:
                    del cache_dict[key]
                    
        except Exception as e:
            logger.error(f"Cache storage failed: {e}")
    
    def update_financial_data(self) -> bool:
        """
        Update financial data from external sources.
        
        Returns:
            True if update successful
        """
        try:
            # PLACEHOLDER: In real implementation, this would:
            # 1. Fetch data from financial APIs (Yahoo Finance, Alpha Vantage, etc.)
            # 2. Process and clean the data
            # 3. Generate embeddings for new documents
            # 4. Update vector database
            
            logger.info("Financial data update completed (mock)")
            return True
            
        except Exception as e:
            logger.error(f"Financial data update failed: {e}")
            return False
    
    def get_database_stats(self) -> Dict[str, Any]:
        """
        Get vector database statistics.
        
        Returns:
            Database statistics dictionary
        """
        try:
            stats = {
                "total_documents": len(self.document_store),
                "news_documents": len([d for d in self.document_store if d.get("category") == "news"]),
                "alert_documents": len([d for d in self.document_store if d.get("category") == "alert"]),
                "vector_db_available": self.vector_db is not None,
                "embedding_model_available": self.embedding_model is not None,
                "cache_sizes": {
                    "news_cache": len(self.news_cache),
                    "alerts_cache": len(self.alerts_cache),
                    "market_cache": len(self.market_data_cache),
                    "stock_evaluation_cache": len(self.stock_evaluation_cache)
                },
                "stock_search_engine_available": self.stock_search_engine is not None
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"Failed to get database stats: {e}")
            return {}
    
    def clear_cache(self, cache_type: Optional[str] = None) -> bool:
        """
        Clear cache data.
        
        Args:
            cache_type: Specific cache to clear, or None for all
            
        Returns:
            True if cleared successfully
        """
        try:
            if cache_type is None or cache_type == "news":
                self.news_cache.clear()
            
            if cache_type is None or cache_type == "alerts":
                self.alerts_cache.clear()
            
            if cache_type is None or cache_type == "market":
                self.market_data_cache.clear()
            
            if cache_type is None or cache_type == "stock_evaluation":
                self.stock_evaluation_cache.clear()
            
            logger.info(f"Cache cleared: {cache_type or 'all'}")
            return True
            
        except Exception as e:
            logger.error(f"Cache clearing failed: {e}")
            return False
    
    def export_database(self, file_path: str) -> bool:
        """
        Export vector database to file.
        
        Args:
            file_path: Path to export file
            
        Returns:
            True if export successful
        """
        try:
            export_data = {
                "documents": self.document_store,
                "export_timestamp": datetime.now().isoformat(),
                "stats": self.get_database_stats()
            }
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"Database exported to {file_path}")
            return True
            
        except Exception as e:
            logger.error(f"Database export failed: {e}")
            return False
    
    def import_database(self, file_path: str) -> bool:
        """
        Import vector database from file.
        
        Args:
            file_path: Path to import file
            
        Returns:
            True if import successful
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                import_data = json.load(f)
            
            documents = import_data.get("documents", [])
            
            # REQUIRED: Clear existing data
            self.document_store.clear()
            if self.vector_db is not None:
                self.vector_db.reset()
            
            # REQUIRED: Import documents
            for doc in documents:
                # Remove old embedding and doc_id to regenerate
                if "embedding" in doc:
                    del doc["embedding"]
                if "doc_id" in doc:
                    del doc["doc_id"]
                
                self.add_document(doc)
            
            logger.info(f"Database imported from {file_path}: {len(documents)} documents")
            return True
            
        except Exception as e:
            logger.error(f"Database import failed: {e}")
            return False

    def count_unread_alerts(self, user_id: str) -> int:
        """
        Count unread alerts for a specific user.
        
        Args:
            user_id: User identifier
            
        Returns:
            Number of unread alerts
        """
        try:
            # PLACEHOLDER: In real implementation, this would query the database
            # For now, return a mock count
            return 0
        except Exception as e:
            logger.error(f"Failed to count unread alerts: {e}")
            return 0

    def mark_all_alerts_read(self, user_id: str) -> bool:
        """
        Mark all alerts as read for a specific user.
        
        Args:
            user_id: User identifier
            
        Returns:
            True if successful
        """
        try:
            # PLACEHOLDER: In real implementation, this would update the database
            logger.info(f"Marked all alerts as read for user {user_id}")
            return True
        except Exception as e:
            logger.error(f"Failed to mark alerts as read: {e}")
            return False

    def mark_alert_read(self, user_id: str, alert_id: int) -> bool:
        """
        Mark a specific alert as read.
        
        Args:
            user_id: User identifier
            alert_id: Alert identifier
            
        Returns:
            True if successful
        """
        try:
            # PLACEHOLDER: In real implementation, this would update the database
            logger.info(f"Marked alert {alert_id} as read for user {user_id}")
            return True
        except Exception as e:
            logger.error(f"Failed to mark alert as read: {e}")
            return False

    def fetch_stock_price_from_api(self, code: str) -> Optional[Dict[str, Any]]:
        """
        Fetch stock price from external API.
        
        Args:
            code: Stock code
            
        Returns:
            Stock price data or None
        """
        try:
            # PLACEHOLDER: In real implementation, this would call a real API
            # For now, return mock data
            mock_price = {
                "code": code,
                "price": 50000 + (hash(code) % 10000),
                "change": 500 + (hash(code) % 1000),
                "change_percent": 1.5 + (hash(code) % 5),
                "volume": 1000000 + (hash(code) % 500000),
                "timestamp": datetime.now().isoformat()
            }
            return mock_price
        except Exception as e:
            logger.error(f"Failed to fetch stock price: {e}")
            return None

    def load_latest_evaluation_data(self) -> bool:
        """
        Load latest stock evaluation data from notebook-based processing.
        
        Returns:
            True if data loaded successfully
        """
        try:
            import os
            from pathlib import Path
            import pandas as pd
            
            # Try to load processed evaluation results
            processed_dir = Path(__file__).parent.parent / "data" / "processed"
            eval_file = processed_dir / "stock_evaluation_results.csv"
            
            if eval_file.exists():
                # Load evaluation results
                eval_df = pd.read_csv(eval_file, dtype={'Ï¢ÖÎ™©ÏΩîÎìú': str})
                eval_df['Ï¢ÖÎ™©ÏΩîÎìú'] = eval_df['Ï¢ÖÎ™©ÏΩîÎìú'].str.zfill(6)
                
                # Add to vector database
                for _, row in eval_df.iterrows():
                    doc = {
                        "title": f"{row['Ï¢ÖÎ™©Î™Ö']} ({row['Ï¢ÖÎ™©ÏΩîÎìú']}) ÌèâÍ∞Ä Ï†ïÎ≥¥",
                        "content": f"{row['Ï¢ÖÎ™©Î™Ö']} ÌèâÍ∞ÄÏ†êÏàò: {row['ÌèâÍ∞ÄÏ†êÏàò']}Ï†ê ({row['Ï¢ÖÌï©ÌèâÍ∞Ä']}), ÌèâÍ∞ÄÏù¥Ïú†: {row['ÌèâÍ∞ÄÏù¥Ïú†']}",
                        "category": "stock_evaluation",
                        "timestamp": datetime.now().isoformat(),
                        "stock_code": row['Ï¢ÖÎ™©ÏΩîÎìú'],
                        "stock_name": row['Ï¢ÖÎ™©Î™Ö'],
                        "evaluation_score": row['ÌèâÍ∞ÄÏ†êÏàò'],
                        "evaluation_grade": row['Ï¢ÖÌï©ÌèâÍ∞Ä'],
                        "relevance_keywords": [row['Ï¢ÖÎ™©Î™Ö'], row['Ï¢ÖÎ™©ÏΩîÎìú'], "ÌèâÍ∞Ä", row['Ï¢ÖÌï©ÌèâÍ∞Ä']]
                    }
                    self.add_document(doc)
                
                logger.info(f"Loaded {len(eval_df)} stock evaluation records")
                return True
            else:
                logger.warning("Stock evaluation results file not found")
                return False
                
        except Exception as e:
            logger.error(f"Failed to load evaluation data: {e}")
            return False
    
    def get_latest_stock_data(self, stock_code: str) -> Optional[Dict[str, Any]]:
        """
        Get latest stock data for a specific stock code.
        
        Args:
            stock_code: Stock code (6 digits)
            
        Returns:
            Stock data dictionary or None
        """
        try:
            # Check cache first
            if stock_code in self.market_data_cache:
                return self.market_data_cache[stock_code]
            
            # Try to load from latest price data
            import os
            from pathlib import Path
            from datetime import datetime
            import pandas as pd
            
            data_dir = Path(__file__).parent.parent / "data" / "raw"
            today = datetime.now().strftime('%Y%m%d')
            
            # Find latest price file
            price_file = data_dir / f"kor_price_{today}.csv"
            if not price_file.exists():
                price_files = list(data_dir.glob("kor_price_*.csv"))
                if price_files:
                    price_file = max(price_files, key=lambda x: x.stat().st_mtime)
            
            if price_file.exists():
                # Load price data
                price_df = pd.read_csv(price_file, dtype={'Ï¢ÖÎ™©ÏΩîÎìú': str})
                price_df['Ï¢ÖÎ™©ÏΩîÎìú'] = price_df['Ï¢ÖÎ™©ÏΩîÎìú'].str.zfill(6)
                
                # Find stock data
                stock_data = price_df[price_df['Ï¢ÖÎ™©ÏΩîÎìú'] == stock_code]
                if not stock_data.empty:
                    stock_info = stock_data.iloc[0].to_dict()
                    
                    # Try to get valuation metrics
                    value_file = data_dir / f"kor_value_{today}.csv"
                    if not value_file.exists():
                        value_files = list(data_dir.glob("kor_value_*.csv"))
                        if value_files:
                            value_file = max(value_files, key=lambda x: x.stat().st_mtime)
                    
                    if value_file.exists():
                        value_df = pd.read_csv(value_file, dtype={'Ï¢ÖÎ™©ÏΩîÎìú': str})
                        value_df['Ï¢ÖÎ™©ÏΩîÎìú'] = value_df['Ï¢ÖÎ™©ÏΩîÎìú'].str.zfill(6)
                        
                        # Get PER and PBR
                        per_data = value_df[(value_df['Ï¢ÖÎ™©ÏΩîÎìú'] == stock_code) & (value_df['ÏßÄÌëú'] == 'PER')]
                        pbr_data = value_df[(value_df['Ï¢ÖÎ™©ÏΩîÎìú'] == stock_code) & (value_df['ÏßÄÌëú'] == 'PBR')]
                        
                        if not per_data.empty:
                            stock_info['PER'] = float(per_data['Í∞í'].iloc[0])
                        if not pbr_data.empty:
                            stock_info['PBR'] = float(pbr_data['Í∞í'].iloc[0])
                    
                    # Cache and return
                    self.market_data_cache[stock_code] = stock_info
                    return stock_info
            
            return None
            
        except Exception as e:
            logger.error(f"Failed to get latest stock data: {e}")
            return None
    
    def get_today_news(self, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        Get today's news.
        
        Args:
            top_k: Number of news items to return
            
        Returns:
            List of news items
        """
        try:
            # PLACEHOLDER: In real implementation, this would fetch from news API
            mock_news = [
                {
                    "title": "ÌïúÍµ≠ Ï¶ùÏãú ÏÉÅÏäπÏÑ∏ ÏßÄÏÜç",
                    "content": "ÏΩîÏä§ÌîºÍ∞Ä 3Ïùº Ïó∞ÏÜç ÏÉÅÏäπÌïòÎ©∞ 2400ÎåÄÎ•º ÌöåÎ≥µÌñàÏäµÎãàÎã§.",
                    "summary": "ÏΩîÏä§Ìîº ÏÉÅÏäπÏÑ∏ ÏßÄÏÜç",
                    "sentiment": "positive",
                    "timestamp": datetime.now().isoformat()
                },
                {
                    "title": "ÎØ∏Íµ≠ Ïó∞Ï§Ä Í∏àÎ¶¨ Ïù∏Ìïò ÏãúÏÇ¨",
                    "content": "ÎØ∏Íµ≠ Ïó∞Î∞©Ï§ÄÎπÑÏ†úÎèÑÍ∞Ä Îã§Ïùå ÌöåÏùòÏóêÏÑú Í∏àÎ¶¨ Ïù∏ÌïòÎ•º Í≤ÄÌÜ†Ìï† Í≤ÉÏù¥ÎùºÍ≥† Î∞úÌëúÌñàÏäµÎãàÎã§.",
                    "summary": "Ïó∞Ï§Ä Í∏àÎ¶¨ Ïù∏Ìïò ÏãúÏÇ¨",
                    "sentiment": "positive",
                    "timestamp": datetime.now().isoformat()
                }
            ]
            return mock_news[:top_k]
        except Exception as e:
            logger.error(f"Failed to get today's news: {e}")
            return []

    def filter_news_by_keywords(self, news_list: List[Dict[str, Any]], keywords: List[str], threshold: float = 0.5) -> List[Dict[str, Any]]:
        """
        Filter news by keywords using similarity.
        
        Args:
            news_list: List of news items
            keywords: List of keywords to filter by
            threshold: Similarity threshold
            
        Returns:
            Filtered news list
        """
        try:
            # PLACEHOLDER: In real implementation, this would use embeddings
            # For now, simple keyword matching
            filtered_news = []
            for news in news_list:
                content = f"{news.get('title', '')} {news.get('content', '')}".lower()
                for keyword in keywords:
                    if keyword.lower() in content:
                        filtered_news.append(news)
                        break
            return filtered_news
        except Exception as e:
            logger.error(f"Failed to filter news by keywords: {e}")
            return news_list 