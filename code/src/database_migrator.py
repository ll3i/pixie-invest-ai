#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Database Migration Tool - CSV to Supabase Migration
- ê¸°ì¡´ data_processing.ipynb ê²°ê³¼ë¥¼ DBë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
- 5ë…„ì¹˜ êµ­ë‚´ì£¼ì‹ ë°ì´í„° + ë¯¸êµ­ì£¼ì‹(yfinance) í†µí•©
- ë°°í¬ í™˜ê²½ ëŒ€ì‘ì„ ìœ„í•œ ì™„ì „í•œ DB ì „í™˜
"""

import pandas as pd
import numpy as np
import os
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import yfinance as yf
from tqdm import tqdm
import time
import math
import pickle

from db_client import get_supabase_client

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DatabaseMigrator:
    """CSV ë°ì´í„°ë¥¼ Supabase PostgreSQL DBë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜"""
    
    def __init__(self):
        self.supabase = get_supabase_client()
        self.batch_size = 500
        
        # ë¯¸êµ­ ì£¼ìš” ì¢…ëª© ë¦¬ìŠ¤íŠ¸ (20ê°œ)
        self.us_stocks = [
            'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA',
            'TSLA', 'META', 'BRK-B', 'JNJ', 'V',
            'WMT', 'JPM', 'MA', 'PG', 'HD',
            'SPY', 'QQQ', 'VTI', 'DIS', 'NFLX'
        ]
    
    def display_table_creation_sql(self):
        """í…Œì´ë¸” ìƒì„± SQL ì¶œë ¥ (Supabaseì—ì„œ ìˆ˜ë™ ì‹¤í–‰ìš©)"""
        
        sql_statements = {
            'korean_stock_prices': '''
            CREATE TABLE korean_stock_prices (
                id BIGSERIAL PRIMARY KEY,
                ticker VARCHAR(6) NOT NULL,
                date DATE NOT NULL,
                open_price NUMERIC(12,2),
                high_price NUMERIC(12,2),
                low_price NUMERIC(12,2),
                close_price NUMERIC(12,2),
                volume BIGINT,
                created_at TIMESTAMP DEFAULT NOW(),
                UNIQUE(ticker, date)
            );
            CREATE INDEX idx_korean_prices_ticker_date ON korean_stock_prices(ticker, date);
            ''',
            
            'us_stock_prices': '''
            CREATE TABLE us_stock_prices (
                id BIGSERIAL PRIMARY KEY,
                ticker VARCHAR(10) NOT NULL,
                date DATE NOT NULL,
                open_price NUMERIC(12,2),
                high_price NUMERIC(12,2),
                low_price NUMERIC(12,2),
                close_price NUMERIC(12,2),
                adj_close NUMERIC(12,2),
                volume BIGINT,
                created_at TIMESTAMP DEFAULT NOW(),
                UNIQUE(ticker, date)
            );
            CREATE INDEX idx_us_prices_ticker_date ON us_stock_prices(ticker, date);
            ''',
            
            'stock_metadata': '''
            CREATE TABLE stock_metadata (
                id BIGSERIAL PRIMARY KEY,
                ticker VARCHAR(10) UNIQUE NOT NULL,
                company_name VARCHAR(200),
                market VARCHAR(20),
                sector VARCHAR(100),
                industry VARCHAR(100),
                country VARCHAR(3),
                currency VARCHAR(3),
                market_cap BIGINT,
                stock_type VARCHAR(20),
                created_at TIMESTAMP DEFAULT NOW(),
                updated_at TIMESTAMP DEFAULT NOW()
            );
            ''',
            
            'financial_statements': '''
            CREATE TABLE financial_statements (
                id BIGSERIAL PRIMARY KEY,
                ticker VARCHAR(6) NOT NULL,
                date DATE NOT NULL,
                account_name VARCHAR(100) NOT NULL,
                value NUMERIC(15,2),
                period_type VARCHAR(1) CHECK (period_type IN ('Y', 'Q')),
                created_at TIMESTAMP DEFAULT NOW(),
                UNIQUE(ticker, date, account_name, period_type)
            );
            CREATE INDEX idx_fs_ticker_date ON financial_statements(ticker, date);
            ''',
            
            'valuation_metrics': '''
            CREATE TABLE valuation_metrics (
                id BIGSERIAL PRIMARY KEY,
                ticker VARCHAR(6) NOT NULL,
                date DATE NOT NULL,
                metric_type VARCHAR(10) NOT NULL,
                value NUMERIC(10,4),
                created_at TIMESTAMP DEFAULT NOW(),
                UNIQUE(ticker, date, metric_type)
            );
            CREATE INDEX idx_valuation_ticker_date ON valuation_metrics(ticker, date);
            ''',
            
            'stock_analysis': '''
            CREATE TABLE stock_analysis (
                id BIGSERIAL PRIMARY KEY,
                ticker VARCHAR(6) NOT NULL,
                company_name VARCHAR(200),
                current_price NUMERIC(12,2),
                market_cap BIGINT,
                revenue_growth NUMERIC(8,4),
                profit_margin NUMERIC(8,4),
                debt_ratio NUMERIC(8,4),
                per_ratio NUMERIC(8,4),
                pbr_ratio NUMERIC(8,4),
                evaluation_score INTEGER,
                evaluation_grade VARCHAR(20),
                evaluation_reasons TEXT,
                created_at TIMESTAMP DEFAULT NOW(),
                updated_at TIMESTAMP DEFAULT NOW(),
                UNIQUE(ticker)
            );
            ''',
            
            'enhanced_news': '''
            CREATE TABLE enhanced_news (
                id BIGSERIAL PRIMARY KEY,
                title TEXT NOT NULL,
                summary TEXT,
                url TEXT UNIQUE,
                published_date TIMESTAMP,
                source VARCHAR(100),
                sentiment_score NUMERIC(5,4),
                keywords TEXT[],
                related_tickers VARCHAR(6)[],
                importance_score INTEGER DEFAULT 0,
                created_at TIMESTAMP DEFAULT NOW()
            );
            CREATE INDEX idx_news_date ON enhanced_news(published_date);
            CREATE INDEX idx_news_tickers ON enhanced_news USING GIN(related_tickers);
            '''
        }
        
        print("ğŸ—„ï¸ Supabase í…Œì´ë¸” ìƒì„± SQL")
        print("=" * 60)
        print("ë‹¤ìŒ SQLì„ Supabase SQL Editorì—ì„œ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ì„¸ìš”:\n")
        
        for table_name, sql in sql_statements.items():
            print(f"-- {table_name.upper()} í…Œì´ë¸”")
            print(sql.strip())
            print("\n" + "-" * 40 + "\n")
    
    def load_korean_data_from_csv(self) -> Dict[str, pd.DataFrame]:
        """ê¸°ì¡´ CSV íŒŒì¼ì—ì„œ í•œêµ­ ì£¼ì‹ ë°ì´í„° ë¡œë“œ"""
        
        data = {}
        csv_files = {
            'ticker': 'kor_ticker_*.csv',
            'prices': 'kor_price_*.csv',
            'financial': 'kor_fs_*.csv',
            'valuation': 'kor_value_*.csv',
            'analysis': 'stock_evaluation_results.csv'
        }
        
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        
        for data_type, pattern in csv_files.items():
            try:
                # ìµœì‹  íŒŒì¼ ì°¾ê¸°
                import glob
                search_paths = [
                    base_dir,
                    os.path.join(base_dir, 'docs'),
                    os.path.join(base_dir, 'data', 'raw'),
                    os.path.join(base_dir, 'data'),
                    os.getcwd(),  # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬
                ]
                
                files = []
                for search_path in search_paths:
                    if os.path.exists(search_path):
                        found_files = glob.glob(os.path.join(search_path, pattern))
                        files.extend(found_files)
                
                if files:
                    latest_file = max(files, key=os.path.getctime)
                    logger.info(f"{data_type} íŒŒì¼ ë°œê²¬: {latest_file}")
                    
                    # ë°ì´í„° ë¡œë“œì‹œ ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”
                    df = pd.read_csv(latest_file, encoding='utf-8-sig')
                    
                    # ì¢…ëª©ì½”ë“œ ì»¬ëŸ¼ ì²˜ë¦¬
                    if 'ì¢…ëª©ì½”ë“œ' in df.columns:
                        df['ì¢…ëª©ì½”ë“œ'] = df['ì¢…ëª©ì½”ë“œ'].astype(str).str.zfill(6)
                    elif 'ticker' in df.columns:
                        df['ticker'] = df['ticker'].astype(str).str.zfill(6)
                    
                    # ë¹ˆ ë°ì´í„°í”„ë ˆì„ ì²´í¬
                    if len(df) == 0:
                        logger.warning(f"{data_type} íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {latest_file}")
                        continue
                    
                    data[data_type] = df
                    logger.info(f"{data_type} ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df):,} ë ˆì½”ë“œ")
                    logger.info(f"ì»¬ëŸ¼: {list(df.columns)}")
                else:
                    logger.warning(f"{data_type} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pattern}")
                    logger.info(f"ê²€ìƒ‰ ê²½ë¡œ: {search_paths}")
                    
            except Exception as e:
                logger.error(f"{data_type} ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                import traceback
                logger.error(traceback.format_exc())
        
        return data
    
    def download_us_stock_data(self, years: int = 5) -> pd.DataFrame:
        """yfinanceë¡œ ë¯¸êµ­ ì£¼ì‹ 5ë…„ì¹˜ ë°ì´í„° ë‹¤ìš´ë¡œë“œ"""
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=years * 365)
        
        all_data = []
        
        logger.info(f"ë¯¸êµ­ ì£¼ì‹ {len(self.us_stocks)}ê°œ ì¢…ëª© ë‹¤ìš´ë¡œë“œ...")
        
        for ticker in tqdm(self.us_stocks, desc="US Stocks"):
            try:
                stock = yf.Ticker(ticker)
                
                # ê°€ê²© ë°ì´í„°
                hist = stock.history(start=start_date, end=end_date)
                if not hist.empty:
                    hist.reset_index(inplace=True)
                    hist['ticker'] = ticker
                    hist['date'] = hist['Date'].dt.date
                    
                    # ì»¬ëŸ¼ í‘œì¤€í™”
                    hist = hist[['ticker', 'date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']]
                    hist.columns = ['ticker', 'date', 'open_price', 'high_price', 'low_price', 
                                  'close_price', 'adj_close', 'volume']
                    
                    all_data.append(hist)
                
                time.sleep(0.1)  # API ì œí•œ ë°©ì§€
                
            except Exception as e:
                logger.warning(f"ë¯¸êµ­ ì£¼ì‹ {ticker} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            logger.info(f"ë¯¸êµ­ ì£¼ì‹ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(combined_df):,} ë ˆì½”ë“œ")
            return combined_df
        
        return pd.DataFrame()
    
    def batch_upsert(self, table_name: str, data: pd.DataFrame) -> bool:
        """ë°°ì¹˜ë¡œ ë°ì´í„°ë¥¼ DBì— upsert"""
        
        if data.empty:
            logger.warning(f"ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {table_name}")
            return False
        
        # NaN ê°’ ì²˜ë¦¬
        data = data.replace({np.nan: None, np.inf: None, -np.inf: None})
        
        total_success = 0
        total_chunks = (len(data) + self.batch_size - 1) // self.batch_size
        
        logger.info(f"{table_name} ì—…ë¡œë“œ ì‹œì‘: {len(data):,} ë ˆì½”ë“œ")
        
        for i in tqdm(range(0, len(data), self.batch_size), desc=f"Uploading {table_name}"):
            chunk = data.iloc[i:i+self.batch_size]
            
            try:
                records = chunk.to_dict('records')
                
                # ë‚ ì§œ í˜•ì‹ ë³€í™˜
                for record in records:
                    for key, value in record.items():
                        if isinstance(value, (pd.Timestamp, np.datetime64)):
                            record[key] = pd.to_datetime(value).strftime('%Y-%m-%d')
                        elif key == 'date' and hasattr(value, 'strftime'):
                            record[key] = value.strftime('%Y-%m-%d')
                
                result = self.supabase.table(table_name).upsert(records).execute()
                
                if result.data:
                    total_success += len(result.data)
                
                time.sleep(0.1)
                
            except Exception as e:
                logger.error(f"ë°°ì¹˜ ì—…ë¡œë“œ ì‹¤íŒ¨ {table_name} [{i}:{i+self.batch_size}]: {e}")
                continue
        
        logger.info(f"{table_name} ì—…ë¡œë“œ ì™„ë£Œ: {total_success:,}/{len(data):,}")
        return total_success > 0
    
    def migrate_korean_prices(self, korean_data: Dict[str, pd.DataFrame]):
        """í•œêµ­ ì£¼ì‹ ê°€ê²© ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""
        
        if 'prices' not in korean_data:
            logger.error("ê°€ê²© ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        prices = korean_data['prices'].copy()
        logger.info(f"ê°€ê²© ë°ì´í„° ì›ë³¸ ì»¬ëŸ¼: {list(prices.columns)}")
        logger.info(f"ê°€ê²© ë°ì´í„° ìƒ˜í”Œ:\n{prices.head()}")
        
        # ì»¬ëŸ¼ëª… í‘œì¤€í™”
        column_mapping = {
            'ì¢…ëª©ì½”ë“œ': 'ticker',
            'ë‚ ì§œ': 'date',
            'ì‹œê°€': 'open_price',
            'ê³ ê°€': 'high_price',
            'ì €ê°€': 'low_price',  
            'ì¢…ê°€': 'close_price',
            'ê±°ë˜ëŸ‰': 'volume'
        }
        
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ë§¤í•‘
        available_mapping = {}
        for old_col, new_col in column_mapping.items():
            if old_col in prices.columns:
                available_mapping[old_col] = new_col
            else:
                logger.warning(f"ì»¬ëŸ¼ '{old_col}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        if not available_mapping:
            logger.error("ë§¤í•‘ ê°€ëŠ¥í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        prices = prices.rename(columns=available_mapping)
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_cols = ['ticker', 'date', 'close_price']
        existing_cols = [col for col in required_cols if col in prices.columns]
        
        if len(existing_cols) < len(required_cols):
            logger.error(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {set(required_cols) - set(existing_cols)}")
            return False
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        final_cols = [col for col in ['ticker', 'date', 'open_price', 'high_price', 'low_price', 'close_price', 'volume'] if col in prices.columns]
        prices = prices[final_cols]
        
        # ë‚ ì§œ ë³€í™˜
        try:
            prices['date'] = pd.to_datetime(prices['date'])
        except Exception as e:
            logger.error(f"ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return False
        
        # ë°ì´í„° íƒ€ì… í™•ì¸ ë° ì •ë¦¬
        for col in ['open_price', 'high_price', 'low_price', 'close_price', 'volume']:
            if col in prices.columns:
                prices[col] = pd.to_numeric(prices[col], errors='coerce')
        
        # NaN ì œê±°
        prices = prices.dropna()
        
        if len(prices) == 0:
            logger.error("ì •ë¦¬ í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        logger.info(f"ìµœì¢… ê°€ê²© ë°ì´í„°: {len(prices):,} ë ˆì½”ë“œ, ì»¬ëŸ¼: {list(prices.columns)}")
        
        return self.batch_upsert('korean_stock_prices', prices)
    
    def migrate_us_prices(self, us_data: pd.DataFrame):
        """ë¯¸êµ­ ì£¼ì‹ ê°€ê²© ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""
        
        if us_data.empty:
            logger.error("ë¯¸êµ­ ì£¼ì‹ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        return self.batch_upsert('us_stock_prices', us_data)
    
    def migrate_metadata(self, korean_data: Dict[str, pd.DataFrame]):
        """ì£¼ì‹ ë©”íƒ€ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""
        
        metadata_records = []
        
        # í•œêµ­ ì£¼ì‹ ë©”íƒ€ë°ì´í„°
        if 'ticker' in korean_data:
            korean_tickers = korean_data['ticker']
            
            for _, row in korean_tickers.iterrows():
                metadata_records.append({
                    'ticker': row['ì¢…ëª©ì½”ë“œ'],
                    'company_name': row.get('ì¢…ëª©ëª…', ''),
                    'market': row.get('ì‹œì¥êµ¬ë¶„', ''),
                    'market_cap': row.get('ì‹œê°€ì´ì•¡', 0),
                    'stock_type': row.get('ì¢…ëª©êµ¬ë¶„', ''),
                    'country': 'KOR',
                    'currency': 'KRW'
                })
        
        # ë¯¸êµ­ ì£¼ì‹ ë©”íƒ€ë°ì´í„°
        for ticker in self.us_stocks:
            try:
                stock = yf.Ticker(ticker)
                info = stock.info
                
                metadata_records.append({
                    'ticker': ticker,
                    'company_name': info.get('longName', ticker),
                    'sector': info.get('sector', ''),
                    'industry': info.get('industry', ''),
                    'market_cap': info.get('marketCap', 0),
                    'country': 'USA',
                    'currency': 'USD'
                })
                
                time.sleep(0.1)
                
            except Exception as e:
                logger.warning(f"ë¯¸êµ­ ì£¼ì‹ {ticker} ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        if metadata_records:
            metadata_df = pd.DataFrame(metadata_records)
            return self.batch_upsert('stock_metadata', metadata_df)
        
        return False
    
    def migrate_financial_statements(self, korean_data: Dict[str, pd.DataFrame]):
        """ì¬ë¬´ì œí‘œ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""
        
        if 'financial' not in korean_data:
            logger.error("ì¬ë¬´ì œí‘œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        fs_data = korean_data['financial'].copy()
        
        # ì»¬ëŸ¼ëª… í‘œì¤€í™”
        column_mapping = {
            'ì¢…ëª©ì½”ë“œ': 'ticker',
            'ê¸°ì¤€ì¼': 'date',
            'ê³„ì •': 'account_name',
            'ê°’': 'value',
            'ê³µì‹œêµ¬ë¶„': 'period_type'
        }
        
        fs_data = fs_data.rename(columns=column_mapping)
        
        # ê¸°ê°„ êµ¬ë¶„ ë³€í™˜ (y -> Y, q -> Q)
        fs_data['period_type'] = fs_data['period_type'].str.upper()
        
        # ë‚ ì§œ ë³€í™˜
        fs_data['date'] = pd.to_datetime(fs_data['date'])
        
        fs_data = fs_data[['ticker', 'date', 'account_name', 'value', 'period_type']]
        
        return self.batch_upsert('financial_statements', fs_data)
    
    def migrate_valuation_metrics(self, korean_data: Dict[str, pd.DataFrame]):
        """ë°¸ë¥˜ì—ì´ì…˜ ì§€í‘œ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        
        if 'valuation' not in korean_data:
            logger.error("ë°¸ë¥˜ì—ì´ì…˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        valuation_data = korean_data['valuation'].copy()
        
        # ì»¬ëŸ¼ëª… í‘œì¤€í™”
        column_mapping = {
            'ì¢…ëª©ì½”ë“œ': 'ticker',
            'ê¸°ì¤€ì¼': 'date',
            'ì§€í‘œ': 'metric_type',
            'ê°’': 'value'
        }
        
        valuation_data = valuation_data.rename(columns=column_mapping)
        
        # ë‚ ì§œ ë³€í™˜
        valuation_data['date'] = pd.to_datetime(valuation_data['date'])
        
        valuation_data = valuation_data[['ticker', 'date', 'metric_type', 'value']]
        
        return self.batch_upsert('valuation_metrics', valuation_data)
    
    def migrate_stock_analysis(self, korean_data: Dict[str, pd.DataFrame]):
        """ì£¼ì‹ ë¶„ì„ ê²°ê³¼ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        
        if 'analysis' not in korean_data:
            logger.error("ë¶„ì„ ê²°ê³¼ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        analysis_data = korean_data['analysis'].copy()
        
        # ì»¬ëŸ¼ëª… í‘œì¤€í™”
        column_mapping = {
            'ì¢…ëª©ì½”ë“œ': 'ticker',
            'ì¢…ëª©ëª…': 'company_name',
            'í˜„ì¬ê°€': 'current_price',
            'ì‹œê°€ì´ì•¡': 'market_cap',
            'ë§¤ì¶œì„±ì¥ë¥ ': 'revenue_growth',
            'ìˆœì´ìµë¥ ': 'profit_margin',
            'ë¶€ì±„ë¹„ìœ¨': 'debt_ratio',
            'PER': 'per_ratio',
            'PBR': 'pbr_ratio',
            'í‰ê°€ì ìˆ˜': 'evaluation_score',
            'ì¢…í•©í‰ê°€': 'evaluation_grade',
            'í‰ê°€ì´ìœ ': 'evaluation_reasons'
        }
        
        analysis_data = analysis_data.rename(columns=column_mapping)
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        columns_to_keep = ['ticker', 'company_name', 'current_price', 'market_cap', 
                          'revenue_growth', 'profit_margin', 'debt_ratio', 'per_ratio', 
                          'pbr_ratio', 'evaluation_score', 'evaluation_grade', 'evaluation_reasons']
        
        analysis_data = analysis_data[columns_to_keep]
        
        return self.batch_upsert('stock_analysis', analysis_data)
    
    def run_migration(self):
        """ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"""
        
        logger.info("ğŸš€ ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
        logger.info("=" * 50)
        
        # 1. í…Œì´ë¸” ìƒì„± SQL í‘œì‹œ
        self.display_table_creation_sql()
        
        confirm = input("\ní…Œì´ë¸” ìƒì„±ì´ ì™„ë£Œë˜ì—ˆë‚˜ìš”? (y/N): ")
        if confirm.lower() not in ['y', 'yes']:
            print("í…Œì´ë¸”ì„ ìƒì„±í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return
        
        # 2. í•œêµ­ ì£¼ì‹ ë°ì´í„° ë¡œë“œ
        logger.info("ğŸ“Š í•œêµ­ ì£¼ì‹ ë°ì´í„° ë¡œë“œ")
        korean_data = self.load_korean_data_from_csv()
        
        # 3. ë¯¸êµ­ ì£¼ì‹ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        logger.info("ğŸ‡ºğŸ‡¸ ë¯¸êµ­ ì£¼ì‹ ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
        us_data = self.download_us_stock_data(years=5)
        
        # 4. ê° í…Œì´ë¸”ë³„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
        migrations = [
            ("í•œêµ­ ì£¼ì‹ ê°€ê²©", lambda: self.migrate_korean_prices(korean_data)),
            ("ë¯¸êµ­ ì£¼ì‹ ê°€ê²©", lambda: self.migrate_us_prices(us_data)),
            ("ì£¼ì‹ ë©”íƒ€ë°ì´í„°", lambda: self.migrate_metadata(korean_data)),
            ("ì¬ë¬´ì œí‘œ", lambda: self.migrate_financial_statements(korean_data)),
            ("ë°¸ë¥˜ì—ì´ì…˜ ì§€í‘œ", lambda: self.migrate_valuation_metrics(korean_data)),
            ("ì£¼ì‹ ë¶„ì„", lambda: self.migrate_stock_analysis(korean_data))
        ]
        
        results = {}
        for name, migration_func in migrations:
            logger.info(f"ğŸ“¤ {name} ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
            try:
                success = migration_func()
                results[name] = "âœ… ì„±ê³µ" if success else "âŒ ì‹¤íŒ¨"
            except Exception as e:
                logger.error(f"{name} ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
                results[name] = f"âŒ ì˜¤ë¥˜: {str(e)}"
        
        # 5. ê²°ê³¼ ìš”ì•½
        logger.info("\n" + "=" * 50)
        logger.info("ğŸ“‹ ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼ ìš”ì•½")
        logger.info("=" * 50)
        
        for name, status in results.items():
            logger.info(f"{name}: {status}")
        
        # 6. DB ìƒíƒœ í™•ì¸
        self.verify_migration()
    
    def verify_migration(self):
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼ ê²€ì¦"""
        
        tables = [
            'korean_stock_prices',
            'us_stock_prices', 
            'stock_metadata',
            'financial_statements',
            'valuation_metrics',
            'stock_analysis'
        ]
        
        logger.info("\nğŸ“Š DB ìƒíƒœ í™•ì¸")
        logger.info("-" * 30)
        
        for table in tables:
            try:
                result = self.supabase.table(table).select('*', count='exact').limit(1).execute()
                count = result.count if hasattr(result, 'count') else 'í™•ì¸ë¶ˆê°€'
                logger.info(f"{table}: {count:,} ë ˆì½”ë“œ" if isinstance(count, int) else f"{table}: {count}")
            except Exception as e:
                logger.info(f"{table}: âŒ ì˜¤ë¥˜ ({e})")
        
        logger.info("\nğŸ‰ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
        logger.info("ì´ì œ ë°°í¬ í™˜ê²½ì—ì„œë„ ì•ˆì •ì ì¸ ë°ì´í„° ì„œë¹„ìŠ¤ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸ”„ MINERVA ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬")
    print("=" * 50)
    print("- ê¸°ì¡´ CSV íŒŒì¼ â†’ Supabase PostgreSQL ë§ˆì´ê·¸ë ˆì´ì…˜")
    print("- í•œêµ­ ì£¼ì‹ 5ë…„ì¹˜ ë°ì´í„° (ê¸°ì¡´ ë…¸íŠ¸ë¶ ê¸°ë°˜)")  
    print("- ë¯¸êµ­ ì£¼ì‹ 5ë…„ì¹˜ ë°ì´í„° (yfinance)")
    print("- ì´ ì˜ˆìƒ ë°ì´í„°ëŸ‰: ~100ë§Œ ë ˆì½”ë“œ")
    print("- ì˜ˆìƒ ì†Œìš”ì‹œê°„: 1-2ì‹œê°„")
    
    migrator = DatabaseMigrator()
    migrator.run_migration()

if __name__ == "__main__":
    main()